import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression

if __name__ == "__main__":
    default=pd.read_csv("../datasets/Default.csv")
    default['default']=default['default'].apply(lambda x: 0 if x=='No' else 1)
    default['student']=default['student'].apply(lambda x: 1 if x=='Yes' else 0)

    X=default.drop('default',axis=1).values
    y=default['default'].values

    X = (X - X.mean(axis=0)) / X.std(axis=0)
    X = np.hstack([np.ones((X.shape[0], 1)), X])

    beta=np.zeros(X.shape[1])  # one beta per feature, including intercept
    lr=0.0001
    n_iter=5000
    tol=1e-6
    prev_loss=float('inf')

    for i in range(n_iter):
        z=X@beta
        p_hat=1/(1+np.exp(-z))
        loss = -np.mean(y * np.log(p_hat + 1e-15) + (1 - y) * np.log(1 - p_hat + 1e-15))
        
        # Check convergence
        if abs(prev_loss - loss) < tol:
            print(f"Converged at iteration {i}, loss = {loss:.6f}")
            break
        
        grad = X.T@(y-p_hat)  # shape: (n_features + 1,)
        beta += lr * grad

    print("----------------------------------------")
    print("beta from scratch:",beta)

    lr = LogisticRegression(penalty=None,fit_intercept=False)
    lr.fit(X,y)

    print("----------------------------------------")
    print("beta from sklearn:",lr.coef_.ravel())
    print("----------------------------------------")
